{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"maD7C0MfjvsO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fA4xhvy5zUWO"},"outputs":[],"source":["!pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEluR6ZlGEq4"},"outputs":[],"source":["!pip install patchify"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"DY_eQw66DsUh","executionInfo":{"status":"ok","timestamp":1716496648570,"user_tz":-330,"elapsed":2,"user":{"displayName":"MAYANK JATAV","userId":"06377763454835745476"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib\n","from matplotlib import pyplot as plt\n","from tqdm.auto import tqdm\n","import time\n","import os\n","from patchify import patchify, unpatchify\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import random_split\n","import timm"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfeNQjm9hala","executionInfo":{"status":"ok","timestamp":1716496649258,"user_tz":-330,"elapsed":3,"user":{"displayName":"MAYANK JATAV","userId":"06377763454835745476"}},"outputId":"1143b372-398e-45fc-9955-952da2f14163"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}],"source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716496649864,"user":{"displayName":"MAYANK JATAV","userId":"06377763454835745476"},"user_tz":-330},"id":"eZ1gkof7FFiE"},"outputs":[],"source":["transform = transforms.Compose([transforms.Resize(384),\n","                                transforms.ToTensor()\n","                                ])"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"15SZlG3DFMUC","executionInfo":{"status":"ok","timestamp":1716496657454,"user_tz":-330,"elapsed":5963,"user":{"displayName":"MAYANK JATAV","userId":"06377763454835745476"}}},"outputs":[],"source":["batch_size = 8\n","fake_dir = '/content/drive/MyDrive/Datasets/FF++/Sample Dataset/C23/Manipulated/DeepFake'\n","real_dir = '/content/drive/MyDrive/Datasets/FF++/Sample Dataset/C23/Original/Original'\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, path, transform=None):\n","        self.transform = transform\n","        self.files = [os.path.join(path, f) for f in os.listdir(path)]\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        file = self.files[idx]\n","        image = Image.open(file)\n","        label = 0.0 if \"Original\" in file else 1.0\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","custom_fake_dataset = CustomImageDataset(fake_dir, transform=transform)\n","\n","total_samples = len(custom_fake_dataset)\n","train_size = 2938 #int(0.7 * total_samples)\n","val_size = 200 #int(0.2 * total_samples)\n","test_size = 100 #total_samples - train_size - val_size\n","rem_size =  total_samples - train_size - val_size - test_size\n","\n","fake_train_subset, fake_val_subset, fake_test_subset, _ = random_split(\n","    custom_fake_dataset, [train_size, val_size, test_size, rem_size]\n",")\n","\n","custom_real_dataset = CustomImageDataset(real_dir, transform=transform)\n","\n","total_samples = len(custom_real_dataset)\n","train_size = 2938 # int(0.7 * total_samples)\n","val_size = 200 # int(0.2 * total_samples)\n","test_size = 100 # total_samples - train_size - val_size\n","rem_size =  total_samples - train_size - val_size - test_size\n","\n","real_train_subset, real_val_subset, real_test_subset, _ = random_split(\n","    custom_real_dataset, [train_size, val_size, test_size, rem_size]\n",")\n","\n","train_dataset = torch.utils.data.ConcatDataset([fake_train_subset, real_train_subset])\n","val_dataset = torch.utils.data.ConcatDataset([fake_val_subset, real_val_subset])\n","test_dataset = torch.utils.data.ConcatDataset([fake_test_subset, real_test_subset])\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n","                                             shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n","                                             shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n","                                             shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INGgDP0GrsWL"},"outputs":[],"source":["print(\"Train Loader Length\", len(train_dataset))\n","print(\"Validation Loader Length\", len(val_dataset))\n","print(\"Test Loader Length\", len(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8qdDaq2ry90"},"outputs":[],"source":["# Display image and label.\n","train_features, train_labels = next(iter(train_loader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0].permute(1, 2, 0)\n","label = train_labels[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.axis('off')\n","plt.tight_layout()\n","plt.show()\n","print(f\"Label: {label}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9xaxGYegmRG"},"outputs":[],"source":["class Xception(nn.Module):\n","  def __init__(self):\n","        super(Xception, self).__init__()\n","        # self.xception_model = tf.keras.applications.xception.Xception(\n","        #     include_top=False,\n","        #     weights='imagenet',\n","        #     input_shape = (384, 384, 3)\n","        # )\n","        # self.global_avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n","        self.global_avg_pool = nn.AvgPool2d(12, 12)\n","        self.xception_model = timm.create_model('xception', pretrained=True)\n","\n","  def forward(self, input):\n","        # x = input.permute(0, 2, 3, 1)\n","        # x = tf.keras.applications.xception.preprocess_input(x)\n","        # output = self.xception_model(x.numpy())\n","        # output = self.global_avg_pool(output.numpy())\n","        # return output.numpy()\n","        output = self.xception_model.forward_features(input)\n","        output = self.global_avg_pool(output)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLJklY77pn69"},"outputs":[],"source":["class PatchModule(nn.Module):\n","  def __init__(self):\n","    super(PatchModule, self).__init__()\n","    self.conv = nn.ModuleList([\n","            nn.ModuleList([\n","                nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n","                for j in range(24)\n","            ])\n","            for i in range(24)\n","        ])\n","\n","  def extract_patches(self, img):\n","    patch_size = 16\n","    stride = 16\n","    patches = img.unfold(2, patch_size, stride).unfold(3, patch_size, stride)\n","    patches = patches.permute(2, 3, 0, 1, 4, 5).squeeze(axis=0)\n","    return patches\n","\n","  def forward(self, x):\n","    patches = self.extract_patches(x)\n","    conv_patches = torch.zeros(24, 24, x.size(0), 3, 16, 16, device=x.device)\n","    for i in range(patches.shape[0]):\n","      for j in range(patches.shape[1]):\n","        for batch_idx in range(patches.shape[2]):\n","          conv_patches[i][j][batch_idx] = self.conv[i][j](patches[i][j][batch_idx])\n","    patches = patches / (torch.max(patches) + 1e-6)\n","    conv_patches = conv_patches / (torch.max(conv_patches) + 1e-6)\n","    output_image = patches * conv_patches\n","    output_image = output_image / (torch.max(output_image) + 1e-6)\n","    return output_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjdSqMPGuy06"},"outputs":[],"source":["pm = PatchModule()\n","outp = pm(train_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPmcMZ7VnhA-"},"outputs":[],"source":["# Display the Self Attention Patched Images\n","# Permute the tensor to bring batch and channel to the front\n","tensor = outp.permute(2, 3, 0, 4, 1, 5)  # [batch, channel, patch_dim1, height, patch_dim2, width]\n","\n","# Reshape to combine patches into full images\n","batch_size, channels, patch_dim1, height, patch_dim2, width = tensor.shape\n","combined_images = tensor.reshape(batch_size, channels, patch_dim1 * height, patch_dim2 * width)\n","\n","# Display the images\n","def show_images(images, custom_width=2, custom_height=2):\n","    batch_size, channels, height, width = images.shape\n","    rows, cols = 2, 4  # Two rows, four columns\n","    fig, axes = plt.subplots(rows, cols, figsize=(8, 6))  # Set custom figure size\n","\n","    for i in range(rows):\n","        for j in range(cols):\n","            img = images[i * cols + j].permute(1, 2, 0).detach().numpy()  # [C, H, W] to [H, W, C]\n","            axes[i, j].imshow(img)\n","            axes[i, j].set_title(f\"Image {i * cols + j + 1}\")\n","            axes[i, j].axis('off')\n","\n","    plt.tight_layout()  # Adjust spacing between subplots\n","    plt.show()\n","\n","# Show the combined images\n","show_images(combined_images)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkSxwrUFg9Ff"},"outputs":[],"source":["class VIT_Encoder(nn.Module):\n","  def __init__(self):\n","    super(VIT_Encoder, self).__init__()\n","    self.vit_model = timm.create_model('vit_small_patch16_384.augreg_in1k', pretrained=True)\n","\n","  def forward(self, x):\n","    output = self.vit_model.forward_features(x)\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TofcESNXaepQ"},"outputs":[],"source":["class Classification_Module(nn.Module):\n","  def __init__(self):\n","    super(Classification_Module, self).__init__()\n","    self.model = nn.Sequential(\n","        nn.Linear(223616, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, 128),\n","        nn.ReLU(),\n","        nn.Linear(128, 1),\n","        nn.Softmax()\n","    )\n","\n","  def forward(self, input):\n","    return self.model(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzUJUFGKkT16"},"outputs":[],"source":["class ViXNet(nn.Module):\n","    def __init__(self):\n","      super(ViXNet, self).__init__()\n","      self.patch_module = PatchModule()\n","      self.vit_encoder = VIT_Encoder()\n","      self.xception = Xception()\n","      self.classification = Classification_Module()\n","\n","    def combine_patches(self, patches):\n","      tensor = patches.permute(2, 3, 0, 4, 1, 5)\n","      batch_size, channels, patch_dim1, height, patch_dim2, width = tensor.shape\n","      combined_images = tensor.reshape(batch_size, channels, patch_dim1 * height, patch_dim2 * width)\n","      return combined_images\n","\n","    def forward(self, x):\n","      patch_module_output = self.combine_patches(self.patch_module(x))\n","      vit_output = torch.flatten(self.vit_encoder(patch_module_output), 1, 2)\n","      xception_output = torch.flatten(self.xception(x), 1, 3)\n","      output = torch.cat((\n","          vit_output,\n","          xception_output\n","          ), dim=1)\n","\n","      output = self.classification(output)\n","      return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDJERcgLl8UI"},"outputs":[],"source":["model = ViXNet().to(device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrVMHKQx0Kzd"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcdA17TJWLw1"},"outputs":[],"source":["def train(model, trainloader, optimizer, loss_fn):\n","    model.train()\n","    print('Training')\n","    train_running_loss = 0.0\n","    train_running_correct = 0\n","    counter = 0\n","    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n","        counter += 1\n","        image, labels = data\n","        labels = torch.unsqueeze(labels, 1)\n","        image = image.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        # forward pass\n","        outputs = model(image)\n","        # calculate the loss\n","        loss = loss_fn(outputs, labels)\n","        train_running_loss += loss.item()\n","        # calculate the accuracy\n","        _, preds = torch.max(outputs.data, 1)\n","        train_running_correct += (preds == labels).sum().item()\n","        # backpropagation\n","        loss.backward()\n","        # update the optimizer parameters\n","        optimizer.step()\n","\n","    # loss and accuracy for the complete epoch\n","    epoch_loss = train_running_loss / counter\n","    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n","    return epoch_loss, epoch_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuuOvnxaWVv1"},"outputs":[],"source":["# validation Of Model\n","def validate(model, testloader, loss_fn):\n","    model.eval()\n","    valid_running_loss = 0.0\n","    valid_running_correct = 0\n","    counter = 0\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n","            counter += 1\n","            image, labels = data\n","            image = image.to(device)\n","            labels = labels.to(device)\n","            outputs = model(image)\n","            loss = loss_fn(outputs, labels)\n","            valid_running_loss += loss.item()\n","            _, preds = torch.max(outputs.data, 1)\n","            valid_running_correct += (preds == labels).sum().item()\n","\n","    epoch_loss = valid_running_loss / counter\n","    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n","    return epoch_loss, epoch_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwG20McIWu1y"},"outputs":[],"source":["train_loss, valid_loss = [], []\n","train_acc, valid_acc = [], []\n","# Author ran for 50 Epochs\n","epochs = 1\n","since = time.time()\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss, train_epoch_acc = train(model, train_loader,\n","                                              optimizer, loss_fn)\n","    valid_epoch_loss, valid_epoch_acc = validate(model, val_loader,\n","                                                 loss_fn)\n","    train_loss.append(train_epoch_loss)\n","    valid_loss.append(valid_epoch_loss)\n","    train_acc.append(train_epoch_acc)\n","    valid_acc.append(valid_epoch_acc)\n","    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n","    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n","    print('-'*50)\n","time_elapsed = time.time() - since\n","print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZzPDw4GXBzg"},"outputs":[],"source":["plt.figure(figsize=(7, 7))\n","plt.plot(\n","    train_acc, color='green', linestyle='-',\n","    label='train accuracy'\n",")\n","plt.plot(\n","    valid_acc, color='blue', linestyle='-',\n","    label='validataion accuracy'\n",")\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.savefig('/content/drive/MyDrive/accuracytl.png')\n","plt.show()\n","# loss plots\n","plt.figure(figsize=(5, 5))\n","plt.plot(\n","    train_loss, color='orange', linestyle='-',\n","    label='train loss'\n",")\n","plt.plot(\n","    valid_loss, color='red', linestyle='-',\n","    label='validataion loss'\n",")\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.savefig('/content/drive/MyDrive/losstl.png')\n","plt.show()\n","# save the final model\n","save_path = 'model_res.pth'\n","torch.save(model.state_dict(), save_path)\n","print('MODEL SAVED...')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}